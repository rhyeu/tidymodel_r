---
title: "tidymodels official get started  chpt 5 A predictive modeling case study"
author: "류성균"
date: '2020 12 27 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

- reference : https://www.tidymodels.org/start/case-study/

### Introduction

```{r}
library(tidymodels)  

# Helper packages
library(readr)       # for importing data
library(vip)         # for variable importance plots
```

### Hotel Bookings data

- reference : [ Antonio, Almeida, and Nunes (2019)](https://www.sciencedirect.com/science/article/pii/S2352340918315191?via%3Dihub)
- data revised : https://gist.github.com/topepo/05a74916c343e57a71c51d6bc32a21ce
- data dictionary : https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11#data-dictionary

```{r}
hotels <-  read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>% 
  mutate_if(is.character, as.factor) 

dim(hotels)
```

- 저자는 '취소 고객'과 '취소하지 않은 고객'간의 데이터 분포의 차이가 있다고 밝힘.
  - such as number of adults/children, room type, meals bought, country of origin of the guests, and so forth
- 이에 이번 분석에서는 취소하지 않은 고객만 분석하기로 함

```{r}
glimpse(hotels)
```

- '아이가 있는가?'는 불균형함
  - `recipes`의 `upsample`, `downsample` 등이나 `themis` 등을 활용할 수 있음
```{r}
hotels %>% 
  count(children) %>% 
  mutate(prop = n / sum(n))
```


### Data splitting & resampling

```{r}
set.seed(123)
splits <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test <- testing(splits)

# training set proportions by children
hotel_other %>% 
  count(children) %>% 
  mutate(prop = n / sum(n))
```


```{r}
# test set proportions by children
hotel_test %>% 
  count(children) %>% 
  mutate(prop = n / sum(n))
```

- `rsample` 함수에서는 10 fold Cross Validation 기능(`vfold_cv()`) 외에 hold-out 기능(`validation_split`)을 쓸 수도 있다.

```{r}
set.seed(234)
val_set <- validation_split(hotel_other,
                            strata = children,
                            prop = 0.8)

val_set
```

### A First model : Penalized logistic regression
- 첫 stpe으로 일부 변수의 계수가 0으로 변하는 LASSO모형을 사용

#### Build the model
```{r}
### mixture를 상수로 두면 변수가 탈락함
lr_mod <- logistic_reg(penalty = tune(), mixture = 1 ) %>% 
  set_engine("glmnet")
```

#### Create the recipe
- date 변수 관련 함수
  - step_date : 일, 월, 달, 요일 변수 추출
  - step_holiday : 특정 휴일에 대한 dummy 변수 생성 
  - step_rm : 변수 제외
- 카테고리 변수 관련
  - step_dummy : 문자변수나 factor 변수를 dummy 변수로 변환
  - step_zv : 분산이 0인 변수를 제외함
  - step_normalize : centering과 scaling을 시행
  
```{r}
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <-   recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

#### Create the workflow
```{r}
lr_workflow <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

#### Create the grid for tunning

```{r}
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid %>% top_n(-5) # lowest penalty values
```

```{r}
lr_reg_grid %>% top_n(5) # highest penalty values
```

#### Train and tune the model

```{r}
lr_res <- lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

```{r}
lr_plot <- lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() +
  geom_line() +
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot
```
- AUC를 보면 8번째(penalty = 0.0005298317) 모델이 좋지만 비슷한 AUC라면 변수가 작은게 좋으므로 이를 고려
```{r}
top_models <- lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty)

top_models
```

```{r}
lr_best <- lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12) # 12번째 모델 고려

lr_best
```

```{r}
lr_auc <- lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

### A Second Model : Tree-based ensemble

#### BUILD THE MODEL AND IMPROVE TRAINING TIME - parallel computing

```{r}
cores <- parallel::detectCores()
cores
```

```{r}
rf_mod <- rand_forest(mtry = tune(),
                     min_n = tune(),
                     trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```

#### Creating the recipe and workflow

- random forest 모델에서는 dummy 변수화 등 feature engineering이 그렇게 중요하지는 않음

```{r}
rf_recipe <- recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date) %>% 
  step_rm(arrival_date)
```

```{r}
rf_workflow <- workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```

#### Train and tune the model

```{r}
rf_mod
```

```{r}
rf_mod %>% parameters()
```

```{r}
set.seed(345)
rf_res <- rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

```{r}
rf_res %>% 
  show_best(metric = "roc_auc")
```

```{r}
autoplot(rf_res)
```

```{r}
rf_best <- rf_res %>% 
  select_best(metric = "roc_auc")

rf_best
```

```{r}
rf_res %>% 
  collect_predictions()
```

- .config에서 best model만 뽑음

```{r}
rf_auc <- rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Random Forest")
```

```{r}
bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1-specificity, y = sensitivity, col = model)) +
  geom_path(lwd = 1.5, alpha = 0.8) + 
  geom_abline(lty = 3) + 
  coord_equal() +
  scale_color_viridis_d(option = "plasma", end = 0.6)
```

### The last fit

```{r}
# the last model
last_rf_mod <-
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <-
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit
```

```{r}
last_rf_fit %>% 
  collect_metrics()
```

```{r}
last_rf_fit %>% 
  pluck(".workflow", 1) %>% 
  pull_workflow_fit() %>% 
  vip(num_features = 20)
```

```{r}
last_rf_fit %>%  
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```

